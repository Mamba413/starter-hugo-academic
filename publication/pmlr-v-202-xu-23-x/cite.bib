@inproceedings{pmlr-v202-xu23x,
 abstract = {Off-policy evaluation (OPE) aims to estimate the return of a target policy using some pre-collected observational data generated by a potentially different behavior policy. In many cases, there exist unmeasured variables that confound the action-reward or action-next-state relationships, rendering many existing OPE approaches ineffective. This paper develops an instrumental variable (IV)-based method for consistent OPE in confounded sequential decision making. Similar to single-stage decision making, we show that IV enables us to correctly identify the target policy's value in infinite horizon settings as well. Furthermore, we propose a number of policy value estimators and illustrate their effectiveness through extensive simulations and real data analysis from a world-leading short-video platform.},
 author = {Xu, Yang and Zhu, Jin and Shi, Chengchun and Luo, Shikai and Song, Rui},
 booktitle = {Proceedings of the 40th International Conference on Machine Learning},
 editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
 pages = {38848--38880},
 pdf = {https://proceedings.mlr.press/v202/xu23x/xu23x.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {An Instrumental Variable Approach to Confounded Off-Policy Evaluation},
 url = {https://proceedings.mlr.press/v202/xu23x.html},
 volume = {202},
 year = {2023}
}

